{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ViniciusBerti/20241_maua_ecm252_intro_git/blob/main/imt_ex4b_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Exercício Avaliativo 2 - 4o Bimestre**"
      ],
      "metadata": {
        "id": "egMY3jx-rhtd"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rYx9D4GZA5o9",
        "cellView": "form"
      },
      "source": [
        "#@title **Identificação do Grupo**\n",
        "\n",
        "#@markdown Nomes completos em ordem alfabética (*\\<nome\\>, \\<RA\\>*)\n",
        "Aluno1 = 'ERICK EIJI NAGAO, 21.00690-3' #@param {type:\"string\"}\n",
        "Aluno2 = 'IGOR IMPROTA MARTINEZ DA SILVA, 21.00834-5' #@param {type:\"string\"}\n",
        "Aluno3 = 'GABRIEL HENRIQUE BACA RADO, 21.01286-5' #@param {type:\"string\"}\n",
        "Aluno4 = 'RYUSKE HIDEAKI SATO, 21.00745-4' #@param {type:\"string\"}\n",
        "Aluno5 = 'VINICIUS DE OLIVEIRA BERTI, 21.01219-9' #@param {type:\"string\"}\n",
        "Aluno6 = 'None' #@param {type:\"string\"}\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Tokenização e Clusterização**\n",
        "\n",
        "## Introdução\n",
        "A clusterização de textos tem uma série de usos, como o agrupamento de mensagens, textos ou notícias com características comuns. Os algoritmos tradicionais de clusterização, entretanto, requerem *features* numéricas e, portanto, a *tokenização* dos documentos.\n",
        "\n",
        "## Este trabalho\n",
        "\n",
        "Neste trabalho você vai analisar fazer a clusterização de mensagens de grupos de notícias e comparar a aderência (*acuracidade de clusterização*) dos agrupamentos obtidos com as classes previamente conhecidas dos grupos.\n",
        "\n",
        "1. **A1 (8.0)** **Tokenização TF-IDF e Clusterização**. Empregue uma tokenização TF-IDF de sua escolha e faça ao menos dois modelos de clusterização (por exemplo k-médias e clusterização hierárquica). Tenha atenção e justifique os parâmetros da Tokenização (n-gramas por exemplo) e da clusterização (métrica de distância!). Analise e compare os resultados com as classes já conhecidas das notícias.\n",
        "\n",
        "2. **A2 (2.0)** **Tokenização LLM e Clusterização**. Empregue agora um modelo LLM para tokenização e repita os procedimentos de clusterização anteriores. Apresente brevemente o modelo e suas referências. Analise e compare os resultados indicando a melhor solução encontrada entre o A1 e A2.\n",
        "\n",
        "3. **A3 (2.0)** **PCA e Clusterização**. Aplique um PCA (0.8) às features (tokens TF-IDF e do LLM) gerando novamente as duas clusterizações selecionadas. Discuta os resultados obtidos.   \n"
      ],
      "metadata": {
        "id": "HH7htSJncUYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Seleção dos Dados para Clusterização\n",
        "\n",
        "São selicionados 5 categorias de mensagens empregando o RA do 1o aluno do grupo (ordem alfabética) como `seed` para seleção."
      ],
      "metadata": {
        "id": "dkfvFsPcyf0x"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RLW2CfMAsV5V",
        "outputId": "febcebcc-2026-4e59-f0c7-e2d785aaac14"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['rec.autos',\n",
              " 'comp.os.ms-windows.misc',\n",
              " 'talk.politics.guns',\n",
              " 'soc.religion.christian',\n",
              " 'misc.forsale']"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "RA = 1115665\n",
        "\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "categories = ['alt.atheism',\n",
        " 'comp.graphics',\n",
        " 'comp.os.ms-windows.misc',\n",
        " 'comp.sys.ibm.pc.hardware',\n",
        " 'comp.sys.mac.hardware',\n",
        " 'comp.windows.x',\n",
        " 'misc.forsale',\n",
        " 'rec.autos',\n",
        " 'rec.motorcycles',\n",
        " 'rec.sport.baseball',\n",
        " 'rec.sport.hockey',\n",
        " 'sci.crypt',\n",
        " 'sci.electronics',\n",
        " 'sci.med',\n",
        " 'sci.space',\n",
        " 'soc.religion.christian',\n",
        " 'talk.politics.guns',\n",
        " 'talk.politics.mideast',\n",
        " 'talk.politics.misc',\n",
        " 'talk.religion.misc']\n",
        "\n",
        "np.random.seed(RA)\n",
        "my_categories =  []\n",
        "\n",
        "for i in np.random.choice(len(categories), 5, replace=False):\n",
        "    my_categories.append(categories[i])\n",
        "\n",
        "my_categories"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = fetch_20newsgroups(\n",
        "    remove=(\"headers\", \"footers\", \"quotes\"),\n",
        "    subset=\"all\",\n",
        "    shuffle=True,\n",
        "    categories=my_categories,\n",
        "    random_state=42,\n",
        ")"
      ],
      "metadata": {
        "id": "wP8Xxitfs1Tk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = dataset.target\n",
        "unique_labels, category_sizes = np.unique(labels, return_counts=True)\n",
        "true_k = unique_labels.shape[0]\n",
        "\n",
        "print(f\"{len(dataset.data)} documents - {true_k} categories\")"
      ],
      "metadata": {
        "id": "CrbhTDhptIRO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BluFtfHuCGzm",
        "cellView": "form"
      },
      "source": [
        "#@markdown\n",
        "#@title **Avaliação**\n",
        "A1 = 7 #@param {type:\"slider\", min:0, max:8, step:1}\n",
        "#@markdown *Justificativa dos parâmetros TF-IDF e clusterização; aplicação correta dos métodos; análise e discussão dos resultados*\n",
        "\n",
        "A2 = 2 #@param {type:\"slider\", min:0, max:2, step:1}\n",
        "#@markdown *Escolha, descrição e referências do modelo LLM; aplicação correta nos dados; análise e discussão da comparação dos resultados*\n",
        "\n",
        "A3 = 1.5 #@param {type:\"slider\", min:0, max:2, step:1}\n",
        "#@markdown *Escolha e aplicação correta da técnica de PCA; análise e discussão dos resultados*\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Gqw7hUZHyle",
        "cellView": "form"
      },
      "source": [
        "#@markdown # **Nota Final**\n",
        "nota = A1 + A2 + A3\n",
        "\n",
        "print(f'Nota final do trabalho {nota :.1f}')\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "alunos = pd.DataFrame()\n",
        "\n",
        "lista_tia = []\n",
        "lista_nome = []\n",
        "\n",
        "for i in range(1,7):\n",
        "  exec(\"if Aluno\" + str(i) + \" !='None':  lista = Aluno\" + str(i) + \".split(','); lista_tia.append(lista[0]); lista_nome.append(lista[1].upper())\")\n",
        "\n",
        "alunos['ra'] = lista_tia\n",
        "alunos['nome'] = lista_nome\n",
        "alunos['nota'] = np.round(nota,1)\n",
        "print()\n",
        "display(alunos)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Parte 1"
      ],
      "metadata": {
        "id": "R2D4nPoeYTEG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
        "from sklearn.metrics import adjusted_rand_score, silhouette_score\n",
        "from sklearn.metrics import homogeneity_score, completeness_score, v_measure_score\n",
        "\n",
        "tfidf = TfidfVectorizer(stop_words='english', max_df=0.5, min_df=5, ngram_range=(1,2))\n",
        "X_tfidf = tfidf.fit_transform(dataset.data)\n",
        "\n",
        "def eval_clustering(X, clusters, name=\"\"):\n",
        "    print(f\"{name}:\")\n",
        "    print(f\"ARI: {adjusted_rand_score(labels, clusters):.3f}\")\n",
        "    print(f\"Silhouette: {silhouette_score(X, clusters):.3f}\")\n",
        "    print(f\"Homogeneity: {homogeneity_score(labels, clusters):.3f}\")\n",
        "    print(f\"Completeness: {completeness_score(labels, clusters):.3f}\")\n",
        "    print(f\"V-Measure: {v_measure_score(labels, clusters):.3f}\\n\")\n",
        "\n",
        "kmeans_tfidf = KMeans(n_clusters=true_k, random_state=42, n_init=20)\n",
        "clusters_kmeans_tfidf = kmeans_tfidf.fit_predict(X_tfidf)\n",
        "eval_clustering(X_tfidf, clusters_kmeans_tfidf, \"TF-IDF + KMeans\")\n",
        "\n",
        "agg_tfidf = AgglomerativeClustering(n_clusters=true_k, linkage='ward')\n",
        "clusters_agg_tfidf = agg_tfidf.fit_predict(X_tfidf.toarray())\n",
        "eval_clustering(X_tfidf.toarray(), clusters_agg_tfidf, \"TF-IDF + Agglomerative\")"
      ],
      "metadata": {
        "id": "NS-cGioUVYre"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nessa primeira parte nós utilizamos a tokenização TF-IDF (Term Frequency–Inverse Document Frequency) para transformar os textos em vetores numéricos.\n",
        "Os parâmetros escolhidos foram stop_words='english', max_df=0.5, min_df=5 e ngram_range=(1,2), pois ajudam a deixar a representação mais equilibrada — removendo palavras muito frequentes, ignorando termos raros e considerando combinações de duas palavras (bigramas).\n",
        "\n",
        "Aplicamos dois algoritmos de clusterização: K-Means e Agglomerative Clustering (linkage = 'ward'). O K-Means foi escolhido pela eficiência e bom desempenho em dados de alta dimensionalidade, enquanto o Agglomerative foi usado para observar possíveis relações hierárquicas entre os textos.\n",
        "\n",
        "Os resultados mostraram que o K-Means apresentou o melhor desempenho, com ARI = 0.316, Homogeneity = 0.491 e V-Measure = 0.526, enquanto o Agglomerative obteve ARI = 0.239.\n",
        "O Silhouette Score, próximo de zero (0.008), é comum em textos representados por TF-IDF, já que os vetores são muito esparsos e os clusters acabam ficando próximos uns dos outros.\n",
        "\n",
        "Em termos de interpretação, o TF-IDF conseguiu separar parcialmente alguns temas (como religião, política e tecnologia), mas ainda ocorreu mistura entre categorias com vocabulário parecido, como rec.autos e misc.forsale.\n",
        "\n",
        "No geral, o TF-IDF funcionou bem como uma primeira abordagem, conseguindo capturar padrões lexicais, mas sem entender o significado das palavras.\n",
        "O K-Means se mostrou o modelo mais adequado nessa etapa, representando melhor os grupos em comparação ao Agglomerative."
      ],
      "metadata": {
        "id": "Iw-TqqVbjEU9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Parte 2"
      ],
      "metadata": {
        "id": "2wfIdz2CjvuT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
        "\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "X_llm = model.encode(dataset.data, show_progress_bar=True)\n",
        "\n",
        "kmeans_llm = KMeans(n_clusters=true_k, random_state=42, n_init=50)\n",
        "clusters_kmeans_llm = kmeans_llm.fit_predict(X_llm)\n",
        "eval_clustering(X_llm, clusters_kmeans_llm, \"LLM + KMeans\")\n",
        "\n",
        "agg_llm = AgglomerativeClustering(\n",
        "    n_clusters=true_k,\n",
        "    metric='cosine',\n",
        "    linkage='average'\n",
        ")\n",
        "clusters_agg_llm = agg_llm.fit_predict(X_llm)\n",
        "eval_clustering(X_llm, clusters_agg_llm, \"LLM + Agglomerative\")"
      ],
      "metadata": {
        "id": "2r41ctALjyrn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nessa segunda parte nós utilizamos um modelo de linguagem (LLM) para gerar representações vetoriais (embeddings) dos textos, substituindo a abordagem de tokenização com TF-IDF.\n",
        "O modelo usado foi o all-MiniLM-L6-v2, da biblioteca Sentence-Transformers, que gera vetores densos de 384 dimensões e é baseado no Sentence-BERT (Reimers & Gurevych, 2019). Esse modelo consegue capturar o significado e o contexto das frases, e não apenas a frequência das palavras.\n",
        "\n",
        "Aplicamos novamente os dois algoritmos de clusterização — K-Means e Agglomerative (métrica = 'cosine', linkage = 'average') — para comparar os resultados com a etapa anterior.\n",
        "\n",
        "O K-Means apresentou um resultado bem superior ao TF-IDF, com ARI = 0.565, Homogeneity = 0.566 e V-Measure = 0.596, mostrando uma separação mais coerente entre os grupos.\n",
        "Já o Agglomerative teve desempenho um pouco inferior, com ARI = 0.378, Homogeneity = 0.414 e V-Measure = 0.504, mas ainda melhor do que na etapa com TF-IDF.\n",
        "O Silhouette Score continuou baixo (entre 0.04 e 0.07), o que é esperado em embeddings densos, já que os vetores ficam mais próximos entre si no espaço vetorial.\n",
        "\n",
        "Esses resultados mostram que os embeddings do LLM representam muito melhor o conteúdo dos textos, agrupando mensagens relacionadas mesmo quando usam vocabulário diferente.\n",
        "Por exemplo, postagens que falam sobre “fé”, “igreja” e “oração” foram agrupadas no mesmo cluster, mesmo sem repetir as mesmas palavras.\n",
        "\n",
        "No geral, a combinação LLM + K-Means apresentou o melhor desempenho até o momento, mostrando que usar representações semânticas melhora bastante a qualidade da clusterização."
      ],
      "metadata": {
        "id": "7kXlmQZHpZVx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Parte 3"
      ],
      "metadata": {
        "id": "B_jBelmuprIL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "X_tfidf_dense = X_tfidf.toarray()\n",
        "pca_tfidf = PCA(n_components=0.8, svd_solver='full', random_state=42)\n",
        "X_tfidf_pca = pca_tfidf.fit_transform(X_tfidf_dense)\n",
        "\n",
        "kmeans_tfidf_pca = KMeans(n_clusters=true_k, random_state=42, n_init=30)\n",
        "clusters_tfidf_pca = kmeans_tfidf_pca.fit_predict(X_tfidf_pca)\n",
        "eval_clustering(X_tfidf_pca, clusters_tfidf_pca, \"TF-IDF + PCA(0.8) + KMeans\")\n",
        "\n",
        "pca_llm = PCA(n_components=0.8, svd_solver='full', random_state=42)\n",
        "X_llm_pca = pca_llm.fit_transform(X_llm)\n",
        "\n",
        "kmeans_llm_pca = KMeans(n_clusters=true_k, random_state=42, n_init=30)\n",
        "clusters_llm_pca = kmeans_llm_pca.fit_predict(X_llm_pca)\n",
        "eval_clustering(X_llm_pca, clusters_llm_pca, \"LLM + PCA(0.8) + KMeans\")"
      ],
      "metadata": {
        "id": "mZcuMmn2qP5e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nessa terceira parte nós aplicamos a técnica de PCA (Principal Component Analysis) sobre os vetores TF-IDF e LLM, mantendo 80% da variância explicada (n_components=0.8).\n",
        "O objetivo dessa etapa foi reduzir a dimensionalidade dos dados para diminuir o ruído, melhorar o desempenho computacional e verificar o impacto dessa redução na qualidade dos agrupamentos.\n",
        "\n",
        "Após aplicar o PCA, repetimos o processo de clusterização com o K-Means, utilizando as mesmas configurações das etapas anteriores.\n",
        "\n",
        "Nos resultados, o TF-IDF + PCA(0.8) apresentou um leve decréscimo nas métricas, com ARI = 0.298, Homogeneity = 0.474 e V-Measure = 0.509, o que mostra uma pequena perda de informação causada pela compressão dos vetores.\n",
        "Já o LLM + PCA(0.8) manteve praticamente o mesmo desempenho da etapa anterior, com ARI = 0.566, Homogeneity = 0.567 e V-Measure = 0.596, mas com menor custo computacional e resultados mais estáveis.\n",
        "\n",
        "O Silhouette Score permaneceu baixo (entre 0.008 e 0.095), o que é esperado em dados textuais, especialmente após a redução de dimensão.\n",
        "\n",
        "De modo geral, o PCA mostrou-se mais útil quando aplicado aos embeddings do LLM, que conseguiram preservar bem a estrutura semântica dos dados mesmo após a redução.\n",
        "No caso do TF-IDF, a diminuição das dimensões acabou removendo termos discriminativos importantes, o que afetou um pouco a separação entre as categorias.\n",
        "\n",
        "Assim, concluímos que a combinação LLM + PCA(0.8) + K-Means apresentou o melhor equilíbrio entre desempenho, estabilidade e eficiência computacional, sendo a mais adequada para representar os textos de forma compacta e coerente."
      ],
      "metadata": {
        "id": "l-PCzvDyrxeA"
      }
    }
  ]
}